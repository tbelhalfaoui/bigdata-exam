{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "exam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKctd3C53cE5"
      },
      "source": [
        "# PDM Big Data course - final exam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyC1KrAMTvSq"
      },
      "source": [
        "**First thing, please share this notebook with me (belhalfaoui@gmail.com).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJYpQo_9-0rt"
      },
      "source": [
        "## 0. Setup\n",
        "The following code downloads and prepares the dataset, and puts it into the `data` folder. It is ready to execute. You will need it for the practical part (2.).\n",
        "\n",
        "Since it takes a couple of minutes to finish, run it now, and start answering the preliminary questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhcEHVN8FgH"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48njZ6Sa8CnO"
      },
      "source": [
        "import pyspark\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiQNVtuR3efv"
      },
      "source": [
        "!wget https://github.com/CSSEGISandData/COVID-19/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEQNyaSe5PId"
      },
      "source": [
        "!unzip -o -q master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c6EnQOaCNSG"
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo3unOPmCnJw"
      },
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "DATA_IN = Path(\"COVID-19-master/csse_covid_19_data/csse_covid_19_daily_reports\")\n",
        "DATA_OUT = Path(\"data\")\n",
        "\n",
        "for file_path in tqdm(list(DATA_IN.iterdir()), desc=\"Preprocessing\"):\n",
        "    if file_path.suffix != '.csv':\n",
        "        shutil.copy(file_path, Path(\"data\") / file_path.name)\n",
        "        continue\n",
        "    df = pd.read_csv(file_path)\n",
        "    month, day, year = file_path.stem.split('.')[0].split('-')\n",
        "    date = f'{year}-{month}-{day}'\n",
        "    if date > '2021-01-20':\n",
        "        continue\n",
        "    df.rename(columns={'Lat': 'Lat_',\n",
        "                       'Province/State': 'Province_State',\n",
        "                       'Country/Region': 'Country_Region',\n",
        "                       'Last Update': 'Last_Update'}, inplace=True)\n",
        "    df = df.replace(',', '', regex=True).replace('\"', '', regex=True)\n",
        "    df = df[['Province_State', 'Country_Region', 'Last_Update',\n",
        "             'Confirmed', 'Deaths', 'Recovered']].fillna(0)\n",
        "   \n",
        "    df['Date'] = date\n",
        "    df.to_csv(Path(\"data\") / file_path.name, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-zR7lyN3cFC"
      },
      "source": [
        "## 1. Preliminary questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU2VkqRGG6bx"
      },
      "source": [
        "---\n",
        "**Note**\n",
        "\n",
        "In this part, you are expected to write Spark code. Among all Spark methods, you may only use RDD methods (`filter`, `map`, `reduceByKey`, etc.).\n",
        "\n",
        "However, if you come up with another Spark code, that uses any other method, but still produces the expected results, you will still get half of the points.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYefOHlB78WB"
      },
      "source": [
        "### 1.1. About _replication_ and _sharding_:\n",
        "* What problem(s) does each of them try to solve?\n",
        "* Give one drawback of each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wepBhKwBbte"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcSZFH5K3cFD"
      },
      "source": [
        "### 1.2. Among the PySpark methods `filter`, `map`, `count` and `take`, which ones are _lazy_ and which ones are _actions_? Which ones transfer data onto the master, and which ones do not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S3-EjLtGNNm"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9CJVV7rKqFa"
      },
      "source": [
        "### 1.3. Here are three separate Spark codes (wheere `sc` is a Spark context). For each of them, tell how many times the `numbers.txt` file will actually be read from the disk? Please explain each answer in a few words.\n",
        "(a)\n",
        "```\n",
        "sc.textFile(\"numbers.txt\")\n",
        "```\n",
        "(b)\n",
        "```\n",
        "sc.textFile(\"numbers.txt\").cache().max()\n",
        "sc.textFile(\"numbers.txt\").cache().count()\n",
        "```\n",
        "(c)\n",
        "```\n",
        "rdd = sc.textFile(\"numbers.txt\")\n",
        "rdd.max()\n",
        "rdd.count()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtFvlkrGfGQ"
      },
      "source": [
        "[YOUR ANSWER HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGzV0Tpt3cFE"
      },
      "source": [
        "## 2. Practical example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ2kchCEM6PT"
      },
      "source": [
        "### 2.1. Read all CSV files from `data` folder into a single Spark RDD, and count the total number of rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alHSs7O_FnDT"
      },
      "source": [
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKMKRYRkIb80"
      },
      "source": [
        "### 2.2. Find the last (most recent) date in the data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Jp1nHLI5LK"
      },
      "source": [
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMqFuSkGKBPZ"
      },
      "source": [
        "### 2.3. Compute the number of deaths per day worldwide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-i3IgEaF_S7"
      },
      "source": [
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcCH3gJrKOMy"
      },
      "source": [
        "### 2.4. Compute the mortality rate per country (over the whole time period).\n",
        "The mortality rate is defined as the number of deaths divided by the number of confirmed cases.\n",
        "\n",
        "NB: The expected answer only reads the CSV files once. But if you come up with a two-pass solution, you will still get half of the points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o-GnmG0EiHa"
      },
      "source": [
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDOo_zk8KkVb"
      },
      "source": [
        "### 3. (bonus points) Same as questions 2.1, 2.2, 2.3 and 2.3 but using DataFrames instead of RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEb3Sj_6RPIN"
      },
      "source": [
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}